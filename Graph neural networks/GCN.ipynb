{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "eHt4iZuVv1Iq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "GtmDXPmORJgp"
   },
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Axiuv57lhl_l"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "iO5QrmzG92C7"
   },
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_embed_dim : int, output_embed_dim : int, activation=torch.nn.Sigmoid):\n",
    "        super(GCN, self).__init__()\n",
    "        # The k-1 embedding dimensions.\n",
    "        self.input_embed_dim = input_embed_dim\n",
    "        # The k embedding dimensions.\n",
    "        self.output_embed_dim = output_embed_dim\n",
    "        # Th enon linearity\n",
    "        self.activation = activation\n",
    "        self.weights = torch.nn.Parameter(torch.rand(self.input_embed_dim, self.output_embed_dim))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weights)\n",
    "\n",
    "    def forward(self, H : torch.Tensor, A : torch.Tensor):\n",
    "        if A.shape[0] != A.shape[1]:\n",
    "          raise Exception(\"GNN layers expects an Adjecancy Matrix(square).\")\n",
    "        if H.shape[0] != A.shape[0]:\n",
    "          raise Exception(\"Shape mismatch between H and A. expected shape (\", A.shape[0], self.input_embed_dim, \") got (\", H.shape[0], \", \", H.shape[1], \")\")\n",
    "        if H.shape[1] != self.input_embed_dim:\n",
    "          raise Exception(\"H expected shape (\", A.shape[0], \", \", self.input_embed_dim, \") got (\", H.shape[0], \", \", H.shape[1], \")\")\n",
    "        A = A + torch.eye(A.shape[0]).to(torch.device('cuda'))\n",
    "        D = torch.sum(A, dim=1)\n",
    "        D_sqrt_inv = torch.diag(torch.pow(torch.sqrt(D), -1))\n",
    "        A_cap = torch.matmul(torch.matmul(D_sqrt_inv, A), D_sqrt_inv).to(torch.device('cuda'))\n",
    "        return self.activation(torch.matmul(A_cap, torch.matmul(H, self.weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "mT1o0WyF4u8P"
   },
   "outputs": [],
   "source": [
    "A = torch.zeros(dataset.x.shape[0], dataset.x.shape[0], requires_grad=False)\n",
    "edge_index = dataset.edge_index\n",
    "for i in range (0, edge_index.shape[1]):\n",
    "  A[edge_index[0, i], edge_index[1, i]] += 1\n",
    "A = A.to(device)\n",
    "H = dataset.x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Ot0Rxu4UpHuf"
   },
   "outputs": [],
   "source": [
    "Ai = A\n",
    "Hi = H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "wH-cLP_PGdnR"
   },
   "outputs": [],
   "source": [
    "class SemiSupervisedClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_embed_dim : int,  num_classes : int, latent_dim = None):\n",
    "        super(SemiSupervisedClassifier, self).__init__()\n",
    "        if latent_dim is None:\n",
    "          latent_dim = input_embed_dim\n",
    "        self.gnn1 = GCN(input_embed_dim, latent_dim, torch.nn.ReLU())\n",
    "        self.gnn2 = GCN(latent_dim, num_classes, torch.nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, H : torch.Tensor, A : torch.Tensor):\n",
    "        return self.gnn2(self.gnn1(H, A), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "t23mVPeIKRPg"
   },
   "outputs": [],
   "source": [
    "model = SemiSupervisedClassifier(dataset.x.shape[1], dataset.num_classes).to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "7BkmrJqLaZW7"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KvHiHt0kBsN",
    "outputId": "8b9f914e-4c59-4698-ed3f-7c9db0da1fbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.4026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9148, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9702, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9805, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9804, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9804, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9929, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(60):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(H, A)\n",
    "    loss = F.nll_loss(out[dataset.train_mask], dataset.y[dataset.train_mask].to(device))\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1z2UlbElnvk",
    "outputId": "c1c1f466-07e5-4a1e-e1da-e8c6388116fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8030\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(H, A).argmax(dim=1)\n",
    "correct = (pred[dataset.test_mask] == dataset.y[dataset.test_mask].to(device)).sum()\n",
    "acc = int(correct) / int(dataset.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
