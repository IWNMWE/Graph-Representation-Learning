{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eHt4iZuVv1Iq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GtmDXPmORJgp"
   },
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GH2y_k-E3-5R"
   },
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_embed_dim : int, output_embed_dim : int, activation=torch.nn.Sigmoid):\n",
    "        super(GNN, self).__init__()\n",
    "        # The k-1 embedding dimensions.\n",
    "        self.input_embed_dim = input_embed_dim\n",
    "        # The k embedding dimensions.\n",
    "        self.output_embed_dim = output_embed_dim\n",
    "        self.activation = activation\n",
    "        # The neighbourhood weights.\n",
    "        self.W_neigh = torch.nn.Parameter(torch.rand(self.input_embed_dim, self.output_embed_dim))\n",
    "        # The self weights.\n",
    "        self.W_self = torch.nn.Parameter(torch.rand(self.input_embed_dim, self.output_embed_dim))\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.W_neigh)\n",
    "        torch.nn.init.xavier_uniform_(self.W_self)\n",
    "\n",
    "    def forward(self, H : torch.Tensor, A : torch.Tensor):\n",
    "        if A.shape[0] != A.shape[1]:\n",
    "          raise Exception(\"GNN layers expects an Adjecancy Matrix(square).\")\n",
    "        if H.shape[0] != A.shape[0]:\n",
    "          raise Exception(\"Shape mismatch between H and A. expected shape (\", A.shape[0], self.input_embed_dim, \") got (\", H.shape[0], \", \", H.shape[1], \")\")\n",
    "        if H.shape[1] != self.input_embed_dim:\n",
    "          raise Exception(\"H expected shape (\", A.shape[0], \", \", self.input_embed_dim, \") got (\", H.shape[0], \", \", H.shape[1], \")\")\n",
    "\n",
    "        return self.activation(torch.matmul(torch.matmul(A, H), self.W_neigh) + torch.matmul(H, self.W_self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKizuPo3AcnF"
   },
   "outputs": [],
   "source": [
    "class GNNSelfLoop(torch.nn.Module):\n",
    "    def __init__(self, input_embed_dim : int, output_embed_dim : int):\n",
    "        super(GNN, self).__init__()\n",
    "        # The k-1 embedding dimensions.\n",
    "        self.input_embed_dim = input_embed_dim\n",
    "        # The k embedding dimensions.\n",
    "        self.output_embed_dim = output_embed_dim\n",
    "\n",
    "        # The shared weights.\n",
    "        self.W = torch.nn.Parameter(torch.rand(self.input_embed_dim, self.output_embed_dim))\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.W)\n",
    "\n",
    "    def forward(self, H : torch.Tensor, A : torch.Tensor):\n",
    "        if A.shape[0] != A.shape[1]:\n",
    "          raise Exception(\"GNN layers expects an Adjecancy Matrix(square).\")\n",
    "        if H.shape[0] != A.shape[0]:\n",
    "          raise Exception(\"Shape mismatch between H and A. expected shape (\", A.shape[0], self.input_embed_dim, \") got (\", H.shape[0], \", \", H.shape[1], \")\")\n",
    "        if H.shape[1] != self.input_embed_dim:\n",
    "          raise Exception(\"H expected shape (\", A.shape[0], \", \", self.input_embed_dim, \") got (\", H.shape[0], \", \", H.shape[1], \")\")\n",
    "        AI = A + torch.eye(A.shape[0])\n",
    "        return self.sigmoid(torch.matmul(torch.matmul(AI, H), self.W_neigh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Axiuv57lhl_l"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mT1o0WyF4u8P"
   },
   "outputs": [],
   "source": [
    "A = torch.zeros(dataset.x.shape[0], dataset.x.shape[0], requires_grad=False)\n",
    "edge_index = dataset.edge_index\n",
    "for i in range (0, edge_index.shape[1]):\n",
    "  A[edge_index[0, i], edge_index[1, i]] += 1\n",
    "A = A.to(device)\n",
    "H = dataset.x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wH-cLP_PGdnR"
   },
   "outputs": [],
   "source": [
    "class SemiSupervisedClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_embed_dim : int,  num_classes : int, latent_dim = None):\n",
    "        super(SemiSupervisedClassifier, self).__init__()\n",
    "        if latent_dim is None:\n",
    "          latent_dim = input_embed_dim\n",
    "        self.gnn1 = GNN(input_embed_dim, latent_dim, torch.nn.ReLU())\n",
    "        self.gnn2 = GNN(latent_dim, num_classes, torch.nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, H : torch.Tensor, A : torch.Tensor):\n",
    "        return self.gnn2(self.gnn1(H, A), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "t23mVPeIKRPg"
   },
   "outputs": [],
   "source": [
    "model = SemiSupervisedClassifier(dataset.x.shape[1], dataset.num_classes).to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7BkmrJqLaZW7"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KvHiHt0kBsN",
    "outputId": "63e6c12f-a4da-4575-f72d-94ce30915eae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.6146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.6617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7300, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.7684, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8460, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8670, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8362, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.9135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8569, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8688, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8763, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8716, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8805, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(-0.8786, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(60):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(H, A)\n",
    "    loss = F.nll_loss(out[dataset.train_mask], dataset.y[dataset.train_mask].to(device))\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1z2UlbElnvk",
    "outputId": "d2829c5b-83d4-4b86-c001-5966c52fa1c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6770\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(H, A).argmax(dim=1)\n",
    "correct = (pred[dataset.test_mask] == dataset.y[dataset.test_mask].to(device)).sum()\n",
    "acc = int(correct) / int(dataset.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
